# -*- coding: utf-8 -*-
"""[P2]CSMODEL_Project_csmodel_people.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zJkYsiGvoz0kHLXDExXKq39lhycAEzj-

Canvas Group: csmodel people

**HOMSSI, Yazan (S12)**

**MATA, Maria Sarah Althea (S12)**

**OLIVARES, Marc Yuri (S12)**

**TIENG, Eiden Bryce (S11)**

**VILLADAREZ, Jorel (S11)**

### Imports and Reading the Dataset
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# for local import
pokedex_df = pd.read_csv('pokedex.csv')

# for Google Drive import
from google.colab import drive

drive.mount('/content/gdrive/')

pokedex_df = pd.read_csv('/content/gdrive/MyDrive/CSMODEL MCO/pokedex.csv')

"""# **Dataset Description**
The dataset chosen is titled [Complete Pokemon Dataset (Updated 16.04.21)](https://www.kaggle.com/datasets/mariotormo/complete-pokemon-dataset-updated-090420). It contains complete information about 1045 Pokemon, from their names and types to their stats, similar to the in-game Pokedex. This dataset was built with web data mining from popular Pokemon information websites, [Pokemon Database](https://pokemondb.net) and [Serebii.net](serebii.net). Each row is a different Pokemon/Pokemon variant, while each column represents the different stats and information of each Pokemon. The dataset contains 1045 rows, or 1045 observations/Pokemon, and 51 columns, or 51 variables/stats.
"""

# Count the number of observations in pokedex_df
num_observations = len(pokedex_df)
# Count the number of variables in pokedex_df
num_variables = pokedex_df.shape[1]

print("Number of observations:", num_observations)
print("Number of variables:", num_variables)

"""## Variables
* *`pokedex_number`: entry number of the Pokemon in the National Pokedex
* *`name`: English name of the Pokemon
* `german_name`: German name of the Pokemon
* `japanese_name`: Original Japanese name of the Pokemon
* *`generation`: The numbered generation which the Pokemon was first introduced
* *`status`: denotes if a Pokemon is Normal, Sub-Legendary, Legendary, or Mythical
* `species`: Category of the Pokemon
* `type_number`: Number of types that the Pokemon has
* *`type_1`: Primary Type of the Pokemon
* *`type_2`: Secondary Type of the Pokemon if it has one
* *`height_m`: Height of the Pokemon in meters
* *`weight_kg`: Weight of the Pokemon in kilograms
* `abilities_number`: number of abilities of the Pokemon
* *`ability_?`: Name of the Pokemon's abilities
* *`ability_hidden`: Name of the hidden ability of the Pokemon if it has one
* *`total_points`: Total number of Base Points of the Pokemon
* *`hp`: Base HP of the Pokemon
* *`attack`: Base Attack of the Pokemon
* *`defense`: Base Defense of the Pokemon
* *`sp_attack`: Base Special Attack of the Pokemon
* *`sp_defense`: Base Special Defense of the Pokemon
* *`speed`: Base Speed of the Pokemon
* *`catch_rate`: Catch Rate of the Pokemon
* `base_friendship`: Base Friendship of the Pokemon
* *`base_experience`: Base experience of a wild Pokemon when caught
* *`growth_rate`: Growth Rate of the Pokemon
* `egg_type_number`: Number of groups where a Pokemon can hatch
* `egg_type_`?: Names of the egg groups where a Pokemon can hatch
* `percentage_male`: percentage of the species that are male. Blank if the Pokemon is genderless
* `egg_cycles`: The number of cycles (255-257 steps) required to hatch an egg of the Pokemon
* `against_?`: Eighteen features that denote the amount of damage taken against an attack of a particular type

*variables that will be used in the study.

# **Data Cleaning**

## **Checking**

### Summary of the Dataframe

A new dataframe called `study_df` will be made that contains only the variables that will be used in the study. Then, the new dataframe's general information will be shown. It displays every variable, the number of non-null entries in each variable, and its data type. We can use this to check for inconsistencies in count and data type.
"""

# Make an array of the names of the variables that will be used
variables = ['pokedex_number', 'name', 'generation', 'status', 'type_1', 'type_2', 'height_m', 'weight_kg', 'ability_1', 'ability_2', 'ability_hidden', 'total_points', 'hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'catch_rate', 'base_experience', 'growth_rate']

# Make a new dataframe using only those variables
study_df = pokedex_df[variables].copy()

study_df.info()

"""We can see from the information above that there are variables that are incomplete when they should be complete, i.e., `catch_rate` and `base_experience`.

Other variables are incomplete because they are either optional or are exceptions.
* **Partner Pikachu**, **Partner Eevee**, and **Eternatus Eternamax** do not have values in `ability_1` because they naturally do not have abilities in-game.
* **Eternatus Eternamax** is the only Pokemon with no `weight_kg` value. [Pokemon Database](https://pokemondb.net/pokedex/eternatus) lists its weight as "—" while [Serebii.net](https://www.serebii.net/pokedex-swsh/eternatus/) lists it as "????kg"

The variables' data types are also all accurate, which means no data type cleaning is necessary.

We can check for formatting and spelling errors in the values of categorical variables.
"""

study_df['generation'].unique()

study_df['status'].unique()

study_df['type_1'].unique()

study_df['type_2'].unique()

study_df['growth_rate'].unique()

"""There are no errors in the formatting and spelling of the values in categorical variables, except for `growth_rate`.

We can check for "suspicious values" like having `-1` as a default value.
"""

print(study_df[study_df['pokedex_number'] < 0])
print(study_df[study_df['pokedex_number'] > 1044])

print(study_df[study_df['height_m'] < 0])
print(study_df[study_df['height_m'] > 100]) # The tallest Pokemon is Eternatus Eternamax, at 100 meters

print(study_df[study_df['weight_kg'] < 0])
print(study_df[study_df['weight_kg'] > 999.9]) # The heaviest Pokemon are Cosmoem and Celesteela, both at 999.9 kg

print(study_df[study_df['total_points'] < 0])
print(study_df[study_df['total_points'] > 1125]) # The Pokemon with the highest stat total is Eternatus Eternamax, at 1125

print(study_df[study_df['hp'] < 0])
print(study_df[study_df['hp'] > 255]) # The Pokemon with the highest HP are Blissey and Eternatus Eternamax, at 255

print(study_df[study_df['attack'] < 0])
print(study_df[study_df['attack'] > 190]) # The Pokemon with the highest Attack is Mega Mewtwo X, at 190

print(study_df[study_df['defense'] < 0])
print(study_df[study_df['defense'] > 250]) # The Pokemon with the highest Defense is Eternatus Eternamax, at 250

print(study_df[study_df['sp_attack'] < 0])
print(study_df[study_df['sp_attack'] > 194]) # The Pokemon with the highest Special Attack is Mega Mewtwo Y, at 194

print(study_df[study_df['sp_defense'] < 0])
print(study_df[study_df['sp_defense'] > 250]) # The Pokemon with the highest Special Defense is Eternatus Eternamax, at 250

print(study_df[study_df['speed'] < 0])
print(study_df[study_df['speed'] > 200]) # The Pokemon with the highest Speed is Regieleki, at 200

print(study_df[study_df['catch_rate'] < 0])
print(study_df[study_df['catch_rate'] > 255]) # The highest possible Catch Rate is 255

print(study_df[study_df['base_experience'] < 0])
print(study_df[study_df['base_experience'] > 608]) # The Pokemon with the highest Base Experience is Blissey, at 608

"""All of the DataFrames to catch Suspicious Values came up empty.

We can also check for duplicate values within variables that should have unique entries.
"""

# Check for duplicates in the 'name' column
name_duplicates = study_df.duplicated(subset=['name'], keep=False)

# Display rows with duplicate names
name_duplicates_df = study_df[name_duplicates]
print(name_duplicates_df[['pokedex_number', 'name']])

"""There are no duplicate entries in the `name` column.

## **Cleaning**

### Cleaning `catch_rate`

The summary showed that there are only 1027 non-null counts for `catch_rate`, when it should be complete at 1045.
"""

# Mask Pokemon with missing catch_rate
no_catch_rate_mask = study_df['catch_rate'].isnull()
# Make a new df out of rows with no catch_rate
no_catch_rate_df = study_df[no_catch_rate_mask]
no_catch_rate_subset = no_catch_rate_df[['name', 'catch_rate']]
no_catch_rate_subset

"""Most of the Pokemon with no `catch_rate` are Galarian Pokemon. Pokemon Database shows that [Galarian Pokemon have the same catch rate as their regular counterparts](https://pokemondb.net/pokedex/meowth).

![](https://drive.google.com/uc?export=view&id=133_v07tVUzJKlkbYSRP5xYkIP0jpOJTX)

![](https://drive.google.com/uc?export=view&id=1eZu2dmbCahuIOHC0aVnerNS5zYIVvyQf)

That means, we can copy the `catch_rate` of the regular Pokemon to their Galarian counterparts with missing values.
"""

# Mask Galarian Pokemon with missing catch_rate
galarian_no_catch_rate_mask = study_df['catch_rate'].isnull() & study_df['name'].str.contains('Galarian')

for index, galarian_pokemon in study_df[galarian_no_catch_rate_mask].iterrows():
    # Extract the base name (without "Galarian")
    base_name = galarian_pokemon['name'].replace('Galarian ', '')

    # Check if the base name matches with a Pokemon
    if not pd.isnull(study_df.loc[study_df['name'] == base_name, 'catch_rate']).all():
        # Copy the catch_rate from the regular Pokemon to the Galarian counterpart
        study_df.loc[index, 'catch_rate'] = study_df.loc[study_df['name'] == base_name, 'catch_rate'].values[0]

# Mask Pokemon with missing catch_rate
no_catch_rate_mask = study_df['catch_rate'].isnull()
# Make a new df out of rows with no catch_rate
no_catch_rate_df = study_df[no_catch_rate_mask]
no_catch_rate_subset = no_catch_rate_df[['name', 'catch_rate']]
no_catch_rate_subset

"""**Morpeko Hangry Mode** is a variant of **Morpeko Full Belly Mode**, and [they both have the same catch rates](https://pokemondb.net/pokedex/morpeko). The same goes for [Zacian Hero of Many Battles](https://pokemondb.net/pokedex/zacian) (with its Crowned Hero form) and [Zamazenta Hero of Many Battles](https://pokemondb.net/pokedex/zamazenta) (with its Crowned Shield form). We will fill their `catch_rate` values similarly to what we did with Galarian Pokemon."""

# Map the variants with no catch_rate to their counterparts
variants_mapping = {
    'Morpeko Hangry Mode': 'Morpeko Full Belly Mode',
    'Zacian Hero of Many Battles': 'Zacian Crowned Sword',
    'Zamazenta Hero of Many Battles': 'Zamazenta Crowned Shield'
}

# Mask Pokemon with missing catch_rate for the specified variants
variant_no_catch_rate_mask = study_df['catch_rate'].isnull() & study_df['name'].isin(variants_mapping.keys())

for index, variant_pokemon in study_df[variant_no_catch_rate_mask].iterrows():
    # Extract the counterpart name from the variants_mapping
    counterpart_name = variants_mapping.get(variant_pokemon['name'])

    # Check if the counterpart name matches with a Pokemon
    if not pd.isnull(study_df.loc[study_df['name'] == counterpart_name, 'catch_rate']).all():
        # Copy the catch_rate from the counterpart Pokemon to the variant
        study_df.loc[index, 'catch_rate'] = study_df.loc[study_df['name'] == counterpart_name, 'catch_rate'].values[0]

# Mask Pokemon with missing catch_rate
no_catch_rate_mask = study_df['catch_rate'].isnull()
# Make a new df out of rows with no catch_rate
no_catch_rate_df = study_df[no_catch_rate_mask]
no_catch_rate_subset = no_catch_rate_df[['name', 'catch_rate']]
no_catch_rate_subset

"""The only Pokemon left is **Eternatus** in its Eternamax form. This Pokemon does not have a catch rate since it is unobtainable.

### Cleaning `base_experience`

The summary showed that there are only 925 non-null counts for `base_experience`, when it should be complete at 1045.
"""

# Mask Pokemon with missing base_experience
no_base_experience_mask = study_df['base_experience'].isnull()
# Make a new df out of rows with no base_experience
no_base_experience_df = study_df[no_base_experience_mask]
no_base_experience_df

"""A lot of the Pokemon with no `base_experience` are Galarian Pokemon. Pokemon Database shows that [Galarian Pokemon have the same base experience as their regular counterparts](https://pokemondb.net/pokedex/ponyta). This means we can copy the `base_experience` of the regular Pokemon to their Galarian counterparts with missing values."""

# Mask Galarian Pokemon with missing base_experience
galarian_no_base_experience_mask = study_df['base_experience'].isnull() & study_df['name'].str.contains('Galarian')

for index, galarian_pokemon in study_df[galarian_no_base_experience_mask].iterrows():
    # Extract the base name (without "Galarian")
    base_name = galarian_pokemon['name'].replace('Galarian ', '')

    # Check if the base name matches with a Pokemon
    if not pd.isnull(study_df.loc[study_df['name'] == base_name, 'base_experience']).all():
        # Copy the base_experience from the regular Pokemon to the Galarian counterpart
        study_df.loc[index, 'base_experience'] = study_df.loc[study_df['name'] == base_name, 'base_experience'].values[0]

# Mask Pokemon with missing base_experience
no_base_experience_mask = study_df['base_experience'].isnull()
# Make a new df out of rows with no base_experience
no_base_experience_df = study_df[no_base_experience_mask]
no_base_experience_df

"""There are still 101 Pokemon with missing `base_experience` values. It would be too time consuming to check all of these and fill them out individually. It is also unlike the case with `catch_rate`, where the Pokemon with missing values are variants of ones with complete values. It can also be observed that all 99 8th Generation Pokemon have missing values.

It is debatable to simply remove all these Pokemon and instead make this study about all Pokemon from Gen I to Gen VII, but it is much preferred to keep them in. Instead, all of the missing values will be imputed with the median `base_experience` value of the dataframe.
"""

# Get the median of non-null base_experience values
median_base_experience = study_df['base_experience'].median()
print("Median Base Experience:", median_base_experience)

# Imput missing base_experience values with the median
study_df['base_experience'].fillna(median_base_experience, inplace=True)

# Mask Pokemon with missing base_experience
no_base_experience_mask = study_df['base_experience'].isnull()
# Make a new df out of rows with no base_experience
no_base_experience_df = study_df[no_base_experience_mask]
print("\nNo Base Experience subset:")
print(no_base_experience_df)

"""### Cleaning `growth_rate`

The summary showed that there are only 1044 non-null counts for `growth_rate`, when it should be complete at 1045.
"""

# Mask Pokemon with missing growth_rate
no_growth_rate_mask = study_df['growth_rate'].isnull()
# Make a new df out of rows with no growth_rate
no_growth_rate_df = study_df[no_growth_rate_mask]
no_growth_rate_df

"""The only Pokemon with a missing `growth_rate` value is Galarian Darmanitan Zen Mode. According to [Pokemon Database](https://pokemondb.net/pokedex/darmanitan), its growth rate is Medium Slow. We can simply set its growth rate to the correct value."""

study_df.loc[study_df['name'] == 'Galarian Darmanitan Zen Mode', 'growth_rate'] = 'Medium Slow'

# Mask Pokemon with missing growth_rate
no_growth_rate_mask = study_df['growth_rate'].isnull()
# Make a new df out of rows with no growth_rate
no_growth_rate_df = study_df[no_growth_rate_mask]
print("No Growth Rate subset:")
print(no_growth_rate_df)

"""# **Exploratory Data Analysis**

## **Question 1:** What is the distribution of Pokemon Primary Types in the dataset?

### Numerical Summary

The distribution of Pokémon Primary Types in the dataset is quantitatively assessed, revealing insights into the diversity and representation of types among the total of 1,045 Pokémon. The analysis employs various statistical measures to elucidate the central tendency and spread of Pokémon counts across different types.

Total Number of Pokémon: 1,045

* Central Tendency:
  * Mean (Average) Number of Pokémon per Type: 58.06. This statistic represents the average count of Pokémon for each primary type, providing an understanding of the typical size of type groups within the Pokémon universe.
  * Median Number of Pokémon per Type: 44. This measure indicates the midpoint in the distribution of Pokémon counts per type, offering insight into the central tendency of the data that is less affected by outliers.
  * Mode Number of Pokémon per Type: 41. The mode reveals the most common count of Pokémon across types, highlighting the predominant group size in the dataset.

* Spread:
  * Standard Deviation: 32.07. This value quantifies the variation or dispersion of Pokémon counts from the mean across types, indicating how spread out the Pokémon types are in terms of their numbers.
  * Maximum Count of Pokémon in a Type: 134 (for Water type), and Minimum Count of Pokémon in a Type: 8 (for Flying type). These figures demonstrate the range of Pokémon counts from the most to the least populous types, underscoring the diversity in type distribution.
  * Range: 126. The range, calculated as the difference between the maximum and minimum counts, provides a simplistic yet effective measure of the overall spread across Pokémon types, illustrating the disparity in representation from the most common to the rarest types.
"""

# Count the occurrences of each Pokemon type
type_counts = study_df['type_1'].value_counts()

# Display the numerical summary
type_counts

"""The numerical summary provides a snapshot of the distribution and diversity of Pokemon across their primary types in the dataset. With a total of 1045 Pokemon spread over 18 types, the dataset showcases a wide range of populations per type, from the water-dominant group with 134 Pokemon to the relatively scarce flying category with just 8. The mean and median values, sitting at 58.06 and 44 respectively, offer insights into the central tendency of the data, suggesting that while some types are densely populated, others are less common. The mode of 41 further highlights the frequency of this particular count across different types.

The standard deviation of 32.07 points to the significant variability in the number of Pokemon per type, underscoring the game designers' efforts to create a balanced yet diverse universe. The substantial range of 126 between the most and least populous types reflects the strategic choices made in the distribution of Pokemon types, affecting gameplay dynamics and player strategies. This diversity not only enriches the Pokemon universe but also ensures a varied and engaging experience for players, encouraging exploration and experimentation with different Pokemon combinations.

### Visualization

The bar chart visualizing the distribution of Pokemon primary types presents a clear and concise comparison across 18 types, with the number of Pokemon depicted along the vertical axis and the types along the horizontal axis. Each type is represented by a blue bar, with their heights indicating the count of Pokemon in each category. The chart is titled "Distribution of Pokemon Primary Types," guiding viewers to understand the variety and prevalence of each type at a glance. Notably, the Water type bar stands out as the tallest, showing it as the most populous, whereas the Flying type has the shortest bar, marking it as the rarest. This visualization effectively communicates the diversity and disparities within the Pokemon universe, making it easy to identify which types are more or less common.
"""

# Plot a bar chart for Pokemon types
plt.figure(figsize=(10, 5))
type_counts.plot(kind='bar', color='blue')
plt.title('Distribution of Pokemon Primary Types')
plt.xlabel('Pokemon Primary Type')
plt.ylabel('Count')
plt.show()

"""## **Question 2:** Is there a relationship between a Pokemon's Legendary status and its stats?

### Numerical Summary

The numerical summary delves into the distribution of base stats (HP, Attack, Defense, Sp. Atk, Sp. Def, Speed, Catch Rate, Base Experience) across Pokémon with different legendary statuses. By employing statistical measures, it contrasts the central tendency and spread of these stats among Pokémon classified under 'Non Legendary' and other legendary categories.

* Central Tendency:
  * Mean: Represents the average value of each base stat for Pokémon within each legendary status category. It sheds light on the "typical" stat value for Pokémon, offering insights into the expected performance metrics of a Pokémon based on its legendary classification.
  * Median: Indicates the middle value of a particular stat among Pokémon in each legendary status category when ordered. This metric is crucial for understanding the central point of the data distribution, especially in scenarios where the data might be skewed, ensuring that outlier values do not disproportionately influence the perception of "average" performance.

* Spread:
  * Standard Deviation (Std): Measures the variability or dispersion of the base stat values around the mean within each legendary status. A higher standard deviation denotes a broader range of values, suggesting that Pokémon within that category exhibit a wide variety of strengths and weaknesses. Conversely, a lower standard deviation indicates that the Pokémon stats are more tightly clustered around the mean, implying more consistency in their attributes.
  * Range: Although not directly calculated in the initial step, the range can be derived from the maximum and minimum values of each stat within the categories. It highlights the difference between the strongest and weakest stat values among Pokémon, providing a quick glimpse into the potential variability and extremes present within each legendary status group.
"""

# Create a copy of the DataFrame
subset_df = study_df[['hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'catch_rate', 'base_experience', 'status', 'total_points']].copy()

# Map 'Mythical', 'Normal', and 'Sub Legendary' to a common category 'Non Legendary'
subset_df['legendary_status'] = subset_df['status'].apply(lambda x: 'Non Legendary' if x in ['Mythical', 'Normal', 'Sub Legendary'] else x)

# Drop the original 'status' column as it's no longer needed
subset_df.drop('status', axis=1, inplace=True)

# Calculate summary statistics for each stat grouped by 'legendary_status'
summary_stats_by_legendary_status = subset_df.groupby('legendary_status').agg(['mean', 'median', 'std'])

# Display the numerical summary
summary_stats_by_legendary_status

"""The numerical summary underscores the distinct gap between Legendary and Non-Legendary Pokémon in terms of battle capabilities and rarity. Legendary Pokémon are designed to be more powerful and challenging to catch, which is reflected in their higher stats across the board and lower catch rates. Their elevated base experience points further highlight their value and the challenge they present to trainers. This data is instrumental in understanding the intrinsic value and designed role of Legendary Pokémon within the game's ecosystem, emphasizing their status as formidable opponents and valuable assets to any trainer's team.

### Visualization
"""

# Create a copy of the DataFrame
subset_df = study_df[['hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'catch_rate', 'base_experience', 'status']].copy()

# Map 'Normal', 'Sub Legendary', and 'Mythical' to 'Non Legendary'
subset_df['status'] = subset_df['status'].apply(lambda x: 'Non Legendary' if x in ['Normal', 'Sub Legendary', 'Mythical'] else x)

# Create box plots for each stat grouped by 'status'
plt.figure(figsize=(16, 10))
for i, stat in enumerate(['hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'catch_rate', 'base_experience']):
    plt.subplot(3, 3, i+1)
    sns.boxplot(x='status', y=stat, data=subset_df, palette='muted', hue='status', showfliers=False, legend=False)
    plt.title(f'Distribution of {stat} by Status')
    plt.xlabel('Status')
    plt.ylabel(stat)

plt.tight_layout()
plt.show()

"""Box plots are used to visualize the distribution of various Pokémon statistics, such as HP, Attack, Defense, Special Attack, Special Defense, Speed, Catch Rate, and Base Experience, categorized by their Legendary status. Each box plot represents the distribution of a specific statistic, comparing the values between Legendary and Non-Legendary Pokémon. Box plots are particularly effective for showcasing the central tendency, spread, and outliers of a dataset, making them ideal for comparing distributions across different groups, as in this case with Legendary and Non-Legendary Pokémon. Through the use of box plots, the visualization offers a comprehensive and easily interpretable comparison of key attributes between these two categories of Pokémon.

## **Question 3:** How does the distribution of base stats (HP, Attack, Defense, Sp. Atk, Sp. Def, Speed) differ across generations?

### Numerical Summary

The numerical summary explores how the distribution of base stats (HP, Attack, Defense, Sp. Atk, Sp. Def, Speed) varies across generations in the Pokemon dataset. It utilizes various statistical measures to compare the central tendency and spread of these stats for Pokemon introduced in each generation.



*   Central Tendency:
 *   Mean: This signifies the average value of each base stat for Pokemon within a specific generation. It provides an understanding of the "typical" stat value for Pokemon introduced in that generation.
 *   Median: This represents the middle value when Pokemon in a generation are ordered by a particular stat. It is particularly useful when the data is skewed, as the mean can be influenced by outliers.

*   Spread:
 *   Standard Deviation: This measures how spread out the data points are around the mean for each stat within a generation. A higher standard deviation indicates a wider range of stat values, while a lower value suggests the data points are closer to the mean.
 *   Range: This calculates the difference between the highest and lowest values of each stat in each generation. It reveals the overall spread of the data but can be sensitive to extreme values.
"""

selected_columns = ["generation", "hp", "attack", "defense", "sp_attack", "sp_defense", "speed"]
dist_df = study_df[selected_columns].copy()

# Function to calculate central tendency and dispersion measures
def calculate_stats(data, stat_name):
  return {
      "mean": data.mean(),
      "median": data.median(),
      "std": data.std(),
      "range": data.max() - data.min()
  }

# Create an empty dictionary to store statistics for each generation and stat
stats_by_generation = {}

# Loop through each generation and calculate statistics for all stats
for generation in dist_df["generation"].unique():
  generation_data = dist_df.loc[dist_df["generation"] == generation]
  stats_by_generation[generation] = {}
  for stat in selected_columns[1:]:
    stats_by_generation[generation][stat] = calculate_stats(generation_data[stat], stat)

# Print the calculated statistics
print("Statistics by generation:")
for generation, stat_data in stats_by_generation.items():
  print(f"\nGeneration {generation}:")
  for stat, stats in stat_data.items():
    print(f"\t{stat}:")
    print(f"\t\tMean: {stats['mean']}")
    print(f"\t\tMedian: {stats['median']}")
    print(f"\t\tStandard Deviation: {stats['std']}")
    print(f"\t\tRange: {stats['range']}")

"""Comparing these statistics across generations, we can gain insights into how the average and distribution of base stats have changed over time. For instance, we might observe if the average HP (mean) for Pokemon increases or decreases with each generation, or if the range of Speed values becomes wider or narrower across generations.

### Visualization

The visualization would employ box plots to effectively portray the distribution of each base stat (HP, Attack, Defense, Sp. Atk, Sp. Def, Speed) across generations. Each box plot would represent the distribution of a specific stat for Pokemon introduced in each generation.



*   X-axis: This axis would represent the different generations of Pokemon.
*   Y-axis: This axis would represent the values of each base stat.
*   Box: The box in each plot would depict the interquartile range (IQR), which encompasses the middle 50% of the data points. The bottom and top edges of the box represent the first and third quartiles (Q1 and Q3), respectively.
*   Median: The line inside the box indicates the median value, which divides the data into two halves with an equal number of data points above and below it.
*   Whiskers: The whiskers extend from the top and bottom of the box, typically up to 1.5 times the IQR. They represent the range of data points that fall outside the interquartile range but are not considered outliers.
*   Outliers: Data points beyond the whiskers are considered outliers and are depicted as individual points outside the box plot.
"""

# Create a figure
plt.figure(figsize=(12, 8))

# Loop through each base stat
for i, stat in enumerate(selected_columns[1:], 1):
  # Create a subplot for each stat
  ax = plt.subplot(2, 3, i)
  sns.boxplot(
      x="generation",
      y=stat,
      showmeans=True,
      data=dist_df
  )
  # Set plot title and labels
  ax.set_title(f"{stat} Distribution by Generation")
  ax.set_xlabel("Generation")
  ax.set_ylabel(stat)

# Adjust layout and display the plot
plt.tight_layout()
plt.show()

"""By examining the box plots, we can visually compare the central tendency (median) and spread (IQR and whiskers) of each base stat across different generations. This allows for an easier identification of trends and potential changes in the distribution of these stats throughout the Pokemon franchise's history.explain

## **Question 4:** Is there a relationship between a Pokemon's generation and its stats?

### Numerical Summary

Statistical measures like central tendency (mean, median, mode) and spread (standard deviation, range) offer key insights into Pokémon stats, revealing patterns in strength and diversity across generations. The mean shows the average stat value, highlighting overall trends in Pokémon power. The median points to the central value, helping identify typical strengths, while the mode indicates the most common stat, shedding light on prevalent characteristics. Standard deviation and range explore the variability among Pokémon, with high values indicating a wide range of abilities and suggesting rich strategic diversity in the game. Together, these statistics provide a nuanced view of Pokémon attributes, enhancing our understanding of their evolution and impact on gameplay.

* Central Tendency
  * Mean: The average value of each stat provides a general idea of the typical stat value for Pokémon in the dataset or within specific categories (e.g., generations, legendary status). A higher mean in stats like HP, Attack, or  Speed for certain generations might indicate that Pokémon from those generations are generally stronger or faster on average.
  * Median: The median gives an insight into the middle value of the dataset for each stat, helping to understand the distribution by indicating the point at which half of the observations fall above and half fall below. This is particularly useful in skewed distributions where the mean might be misleading.
  * Mode: The most frequently occurring value in the dataset for each stat. While not always applicable to continuous data like Pokémon stats, the mode can be useful in categorical data or discrete data analyses.
* Spread
  * Standard Deviation: This measures the amount of variation or dispersion of a set of values. A high standard deviation for a stat across generations could suggest that there is a wide variety of strengths and weaknesses among Pokémon in that generation, indicating diverse game design and strategy.
  * Range: The difference between the highest and lowest values. A wide range in stats like Attack or Defense could indicate the presence of both very powerful and very weak Pokémon within a generation or category, highlighting the diversity in Pokémon characteristics.
"""

selected_columns = ["generation", "hp", "attack", "defense", "sp_attack", "sp_defense", "speed"]
rel_df = study_df[selected_columns]

def calculate_stats(data, stat_name):
    return {
        "mean": data.mean(),
        "median": data.median(),
        "std": data.std(),
        "range": data.max() - data.min()
    }


stats_by_stat = {}


for stat in selected_columns[1:]:
    stats_by_stat[stat] = calculate_stats(rel_df[stat], stat)


print("Overall Statistics:")
for stat, stats in stats_by_stat.items():
    print(f"{stat}:")
    print(f"  Mean: {stats['mean']:.2f}")
    print(f"  Median: {stats['median']}")
    print(f"  Standard Deviation: {stats['std']:.2f}")
    print(f"  Range: {stats['range']}")

"""**Across Generations:** If the mean and median values of stats such as HP, Attack, and Speed gradually increase across generations, this could suggest that newer Pokémon are designed to be stronger or faster, possibly reflecting changes in game design philosophy or an intention to keep the game challenging for players.

**Legendary Status:** A significantly higher mean and lower standard deviation in stats for Legendary Pokémon compared to Non-Legendary ones would highlight their intended role as exceptionally powerful creatures within the game. The lower standard deviation suggests that Legendary Pokémon consistently maintain high stats, underscoring their rarity and strength.

**Predicting Generation:** If certain generations have distinctive statistical signatures—such as a particular generation having consistently higher Speed stats—this could potentially be used as a predictive feature in modeling. However, significant overlap in the range and standard deviation of stats across generations might indicate that predicting a Pokémon's generation based on stats alone could be challenging without more distinct or additional features.

### Visualization

The box plot offers a concise visual representation of how Pokémon stats vary across different generations. Each box illustrates the distribution of stats, with the line inside representing the median value. The whiskers show the range of the data, while outliers are indicated as individual points beyond the whiskers. This visualization facilitates a comparative analysis of stats between generations, aiding in identifying trends and potential patterns that may assist in predicting a Pokémon's generation based on its stats.
"""

melted_df = study_df.melt(id_vars=["generation"], value_vars=["hp", "attack", "defense", "sp_attack", "sp_defense", "speed"],
                    var_name="Stat", value_name="Value")

plt.figure(figsize=(12, 6))
sns.boxplot(data=melted_df, x="Stat", y="Value", hue="generation")
plt.title("Distribution of Overall Pokémon Stats Across Generations")
plt.ylabel("Stat Value")
plt.xlabel("Stat Type")
plt.legend(title="Generation", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()

plt.show()

"""The box plot visualization succinctly illustrates the evolution and variability of Pokémon stats—HP, Attack, Defense, Sp. Attack, Sp. Defense, and Speed—across generations. It reveals trends in stat distribution, such as increases or decreases in median values, indicating shifts in gameplay strategy or game design over time. By comparing stats within generations, it highlights which attributes are emphasized and the strategic diversity offered by the Pokémon roster. Outliers point to exceptional Pokémon that deviate from the norm, showcasing design choices for unique gameplay experiences. The spread and range of the box plots reflect the diversity of Pokémon strengths and weaknesses within each generation, underscoring the series' evolving complexity and depth. This visualization offers valuable insights into the strategic and design philosophies guiding the Pokémon series, making it a useful tool for players and enthusiasts alike to understand the dynamics at play across different eras of the franchise.

# **Research Question: Can we predict a Pokémon's generation based on its stats?**

This research question extends the EDA's questions exploration into predictive modeling. We will be taking the exploration into stats and generations further by attempting predictions based on observed patterns within these variables.

Our exploration revealed that throughout the years, there have been trends of changes in stats between generations, such as increases or decreases in median values, indicating shifts in game development and design philosophies over time. We compared stats within generations and it highlighted which attributes are emphasized and the difference of intended strategies per generation.

This research question is significant as it offers the opportunity to uncover the underlying mechanisms driving the evolution and design of Pokémon across different generations. Additionally, it has practical implications for game developers, researchers, and players interested in understanding the design philosophy and gameplay balancing of a successful franchise and how it evolved over the year, possibly to expound on it or create other variations of the idea.

# **Data Pre-Processing**

`preprocess_df` will first be created out of the needed variables of `study_df`. The needed variables are the target variable, `generation`, and the input variables, which will be all of the stats variables.

The following data pre-processing steps will be applied:

*   Replace Eternatus Eternamax's `growth_rate`, the only NaN value, with `0`
*   Convert `growth_rate` into numerical values
*   Separate input variables (stats) from the target variable (`generation`)
*   Scale numerical variables using mean centering
*   Split the dataset into training and testing sets
"""

from sklearn.model_selection import train_test_split

preprocess_df = study_df[['pokedex_number', 'name', 'generation', 'total_points', 'hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'catch_rate', 'base_experience', 'growth_rate']].copy()
preprocess_df

# Replace Eternatus Eternamax's 'growth_rate' value with 0
preprocess_df.fillna(0, inplace=True)

growth_rate_mapping = {
    'Erratic': 6,
    'Fast': 5,
    'Medium Fast': 4,
    'Medium Slow': 3,
    'Slow': 2,
    'Fluctuating': 1
}

# Convert 'growth_rate' into numerical values
preprocess_df['growth_rate'] = preprocess_df['growth_rate'].map(growth_rate_mapping)

# Separate input variables from the target variable
X = preprocess_df.drop(columns=['generation'])
y = preprocess_df['generation']

# Scale numerical variables using mean centering
numerical_features = [col for col in X.columns if X[col].dtype != 'object']
X_scaled = X.copy()
for column in numerical_features:
    mean_val = X[column].mean()
    X_scaled[column] = X[column] - mean_val  # Subtract the mean value

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Verify correct splitting
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""# **Data Modelling**

**Model Selected:** Random Forests

Model_df Creation: A new DataFrame, model_df, is created from the preprocessed

*   Create a list of feature names, excluding `name`. This is because `name` is a categorical variable that doesn't contribute to the model's ability to predict the generation of a Pokémon.

*   Extract the feature matrix with the `name` column excluded. This matrix, `X_train_subset` and `X_test_subset`, contains the values of the features that the model will learn from.

*   Instantiate: `RandomForestClassifier`. This is the model used for training and prediction.

  *   Training: Fit the `RandomForestClassifier` on the training data. This is where the model learns the patterns in the data.

  *   Prediction: Use the trained `RandomForestClassifier` to make predictions on the test set. These predictions will be compared to the actual values to evaluate the model's performance.


*   Evaluate the model using `accuracy_score`, `classification_report`, and `confusion_matrix`


*   Compile the results in a DataFrame, which includes the Pokémon's name, its actual generation, and the generation predicted by the model.



   Once done, we filter the DataFrame to show only the Pokémon that were incorrectly classified, sort them by the actual generation, and display the sorted DataFrame. This gives us insights into where our model is making mistakes.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Create a list of feature names excluding 'name'
feature_names = X_train.columns[X_train.columns != 'name']

# Extract the feature matrix with the 'name' column excluded
X_train_subset = X_train[feature_names]
X_test_subset = X_test[feature_names]

# Instantiation of learning model
rf = RandomForestClassifier(random_state=42)

# fit the decision tree on the training data
rf.fit(X_train_subset, y_train)

# Make predictions on the test set using the trained model
y_pred_tree = rf.predict(X_test_subset)

# Evaluate the Random Forest model
accuracy_tree = accuracy_score(y_test, y_pred_tree)
conf_matrix_tree = confusion_matrix(y_test, y_pred_tree)
classification_rep_tree = classification_report(y_test, y_pred_tree)

print(f"Random Forest Accuracy: {accuracy_tree}")
print(f"\nRandom Forest Confusion Matrix:\n{conf_matrix_tree}")
print(f"\nRandom Forest Classification Report:\n{classification_rep_tree}")

results_df = pd.DataFrame({
    'Name': X_test['name'],
    'Actual Generation': y_test,
    'Predicted Generation': y_pred_tree
})

# Filter the dataframe to show only the incorrectly classified Pokémon
incorrectly_classified_df = results_df[results_df['Actual Generation'] != results_df['Predicted Generation']]

# Sort the dataframe by the actual generation
incorrectly_classified_df_sorted = incorrectly_classified_df.sort_values(by='Actual Generation')

# Display the sorted dataframe
print(f"Incorrectly Classified Pokemon:\n{incorrectly_classified_df_sorted}")

"""# **Statistical Inference (BASED ON THE MODEL)**

**Null Hypothesis:** There is no significant relationship between predicted and actual generations.

**Alternative Hypothesis:** There is a significant relationship between predicted and actual generations.

**Statistical Test:** Test of Independence, 2 categorical variables

**Determine Significance Level:** 0.05
"""

from scipy.stats import chi2_contingency

# Create a contingency table
contingency_table = pd.crosstab(results_df['Predicted Generation'], results_df['Actual Generation'])

# Perform chi-square test of independence
chi2, p_value, *_ = chi2_contingency(contingency_table)

# Define significance level
alpha = 0.05

# Print chi-square statistic and p-value
print(f"Chi-square statistic: {chi2}")
print(f"P-value: {p_value}")

# Compare p-value to significance level
if p_value < alpha:
    print("Reject null hypothesis: There is a significant relationship between predicted and actual generations.")
else:
    print("Fail to reject null hypothesis: There is no significant relationship between predicted and actual generations.")

"""# **Statistical Inference**

**Null Hypothesis:** There is no relationship between a Pokémon's stats and its generation. the stats of a Pokémon do not provide any information about which generation it belongs to.

**Alternative Hypothesis:** There is a relationship between a Pokémon's stats and its generation. The stats of a Pokémon can be used to predict which generation it belongs to.

**Statistical Test:** ANOVA, testing one categorical variable(generation) and one numeric variable(total_points)

**Determine Significance Level:** 0.05
"""

from scipy.stats import f_oneway

# Perform one-way ANOVA
f_statistic, p_value = f_oneway(*[preprocess_df[preprocess_df['generation'] == gen]['total_points'] for gen in preprocess_df['generation'].unique()])

# Print results
print(f"F-statistic: {f_statistic}")
print(f"P-value: {p_value}")

# Compare p-value to significance level
alpha = 0.05
if p_value < alpha:
    print("Reject null hypothesis: There is a significant relationship between generation and total points.")
else:
    print("Fail to reject null hypothesis: There is no significant relationship between generation and total points.")

"""# **Insights and Conclusions**

**Predictive Modeling**

The Random Forest model demonstrated a remarkable accuracy of 97.13% in predicting Pokémon generations based on stats, highlighting the strong predictive power of statistical data in determining generational attributes.

**Statistical Inference**

The study utilized two main statistical tests:

*  The **chi-square test** yielded a statistic of 1363.47 with a p-value of approximately 2.696 × 10^-253, strongly rejecting the null hypothesis and indicating a significant relationship between predicted and actual generations.

*   The **ANOVA test** showed an F-statistic of 2.0186 and a p-value of 0.049975, further supporting the hypothesis that Pokémon stats are significantly related to their generation.

**Discussion**

The study's findings underscore the effectiveness of using Pokémon stats for generational prediction, reflecting the franchise's evolutionary game design strategies. These insights have profound implications for game developers, players, and researchers, offering a deeper understanding of the dynamics at play in the Pokémon series.

**Implications**

*   **Game Development and Design:** These findings offer insights into how Pokémon design philosophies have evolved across generations, highlighting shifts in emphasis on different stats. Game developers and designers can use this knowledge to create balanced and engaging future generations of Pokémon.

*   **Strategic Gameplay:** Players and enthusiasts can leverage these insights for strategic team building, especially in competitive play where understanding generational strengths and weaknesses could be pivotal.

*   **Future Research:** While the current model demonstrates high predictive accuracy, further research could explore more nuanced relationships within the data, including how specific stats or other Pokémon characteristics like type or abilities contribute to generational differences.

**Conclusion**

In conclusion, the statistical and modeling analyses conducted provide strong evidence that a Pokémon's generation can be predicted based on its stats. This conclusion not only answers the research question affirmatively but also opens up avenues for deeper exploration into the dynamics of game development, strategic gameplay, and the cultural impact of the Pokémon franchise.
"""